{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"mount_file_id":"1rrO7kOCGLab0k1cIsI1WMIJ-6L5G2s2U","authorship_tag":"ABX9TyPtq3oavX2cjA3+JaRe9z95"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","source":["%cd /content/drive/MyDrive/"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"I6X5S_nlxaX4","executionInfo":{"status":"ok","timestamp":1671949374049,"user_tz":-420,"elapsed":17,"user":{"displayName":"Nguyễn Đức Vũ","userId":"07288779277842550568"}},"outputId":"f623eb9c-0be6-487c-e81f-a51ff86f7b05"},"execution_count":1,"outputs":[{"output_type":"stream","name":"stdout","text":["/content/drive/MyDrive\n"]}]},{"cell_type":"code","source":["%cd VietInfoUITNLP/vietinfo_uitnlp_api"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"YxkLfrbJ8nSm","executionInfo":{"status":"ok","timestamp":1671949403323,"user_tz":-420,"elapsed":7,"user":{"displayName":"Nguyễn Đức Vũ","userId":"07288779277842550568"}},"outputId":"6a5df07c-2dd5-44bc-8868-8ad9c61cb7ad"},"execution_count":2,"outputs":[{"output_type":"stream","name":"stdout","text":["/content/drive/MyDrive/VietInfoUITNLP/vietinfo_uitnlp_api\n"]}]},{"cell_type":"code","source":["!unzip vietinfo_uitnlp_data.zip"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"V7cFNWqx81dV","executionInfo":{"status":"ok","timestamp":1671949464167,"user_tz":-420,"elapsed":43316,"user":{"displayName":"Nguyễn Đức Vũ","userId":"07288779277842550568"}},"outputId":"40446b76-32a7-4a41-af10-e6b597106178"},"execution_count":3,"outputs":[{"output_type":"stream","name":"stdout","text":["Archive:  vietinfo_uitnlp_data.zip\n","   creating: vietinfo_uitnlp_data/\n","   creating: vietinfo_uitnlp_data/env_docker_cpu/\n","  inflating: vietinfo_uitnlp_data/env_docker_cpu/odbcinst.ini  \n","  inflating: vietinfo_uitnlp_data/env_docker_cpu/Dockerfile  \n","   creating: vietinfo_uitnlp_data/data/\n","   creating: vietinfo_uitnlp_data/data/sogtvt_new_fold_4/\n","  inflating: vietinfo_uitnlp_data/data/sogtvt_new_fold_4/vanbansudung_ids.txt  \n","   creating: vietinfo_uitnlp_data/data/sogtvt_new_fold_4/part_1/\n","  inflating: vietinfo_uitnlp_data/data/sogtvt_new_fold_4/part_1/documents.json  \n","  inflating: vietinfo_uitnlp_data/data/sogtvt_new_fold_4/part_1/test.json  \n","  inflating: vietinfo_uitnlp_data/data/sogtvt_new_fold_4/part_1/dev.json  \n","  inflating: vietinfo_uitnlp_data/data/sogtvt_new_fold_4/part_1/train.json  \n","   creating: vietinfo_uitnlp_data/data/sogtvt_new_fold_5/\n","  inflating: vietinfo_uitnlp_data/data/sogtvt_new_fold_5/vanbansudung_ids.txt  \n","   creating: vietinfo_uitnlp_data/data/sogtvt_new_fold_5/part_1/\n","  inflating: vietinfo_uitnlp_data/data/sogtvt_new_fold_5/part_1/documents.json  \n","  inflating: vietinfo_uitnlp_data/data/sogtvt_new_fold_5/part_1/test.json  \n","  inflating: vietinfo_uitnlp_data/data/sogtvt_new_fold_5/part_1/dev.json  \n","  inflating: vietinfo_uitnlp_data/data/sogtvt_new_fold_5/part_1/train.json  \n","   creating: vietinfo_uitnlp_data/data/sogtvt_new/\n","  inflating: vietinfo_uitnlp_data/data/sogtvt_new/vanbansudung_ids.txt  \n","  inflating: vietinfo_uitnlp_data/data/sogtvt_new/data_visualization.json  \n","   creating: vietinfo_uitnlp_data/data/sogtvt_new/part_1/\n","  inflating: vietinfo_uitnlp_data/data/sogtvt_new/part_1/documents.json  \n","  inflating: vietinfo_uitnlp_data/data/sogtvt_new/part_1/test.json  \n","  inflating: vietinfo_uitnlp_data/data/sogtvt_new/part_1/dev.json  \n","  inflating: vietinfo_uitnlp_data/data/sogtvt_new/part_1/train.json  \n","  inflating: vietinfo_uitnlp_data/data/sogtvt_new/config.json  \n","  inflating: vietinfo_uitnlp_data/data/sogtvt_new/data_visualization.html  \n","   creating: vietinfo_uitnlp_data/data/sogtvt_new_fold_2/\n","  inflating: vietinfo_uitnlp_data/data/sogtvt_new_fold_2/vanbansudung_ids.txt  \n","   creating: vietinfo_uitnlp_data/data/sogtvt_new_fold_2/part_1/\n","  inflating: vietinfo_uitnlp_data/data/sogtvt_new_fold_2/part_1/documents.json  \n","  inflating: vietinfo_uitnlp_data/data/sogtvt_new_fold_2/part_1/test.json  \n","  inflating: vietinfo_uitnlp_data/data/sogtvt_new_fold_2/part_1/dev.json  \n","  inflating: vietinfo_uitnlp_data/data/sogtvt_new_fold_2/part_1/train.json  \n","   creating: vietinfo_uitnlp_data/data/sogtvt_old/\n","  inflating: vietinfo_uitnlp_data/data/sogtvt_old/vanbansudung_ids.txt  \n","  inflating: vietinfo_uitnlp_data/data/sogtvt_old/data_visualization.json  \n","   creating: vietinfo_uitnlp_data/data/sogtvt_old/part_1/\n","  inflating: vietinfo_uitnlp_data/data/sogtvt_old/part_1/test.json  \n","  inflating: vietinfo_uitnlp_data/data/sogtvt_old/part_1/dev.json  \n","  inflating: vietinfo_uitnlp_data/data/sogtvt_old/part_1/train.json  \n","  inflating: vietinfo_uitnlp_data/data/sogtvt_old/config.json  \n","  inflating: vietinfo_uitnlp_data/data/sogtvt_old/data_visualization.html  \n","   creating: vietinfo_uitnlp_data/data/sogtvt_new_fold_3/\n","  inflating: vietinfo_uitnlp_data/data/sogtvt_new_fold_3/vanbansudung_ids.txt  \n","   creating: vietinfo_uitnlp_data/data/sogtvt_new_fold_3/part_1/\n","  inflating: vietinfo_uitnlp_data/data/sogtvt_new_fold_3/part_1/documents.json  \n","  inflating: vietinfo_uitnlp_data/data/sogtvt_new_fold_3/part_1/test.json  \n","  inflating: vietinfo_uitnlp_data/data/sogtvt_new_fold_3/part_1/dev.json  \n","  inflating: vietinfo_uitnlp_data/data/sogtvt_new_fold_3/part_1/train.json  \n","   creating: vietinfo_uitnlp_data/data/sogtvt_new_fold_1/\n","  inflating: vietinfo_uitnlp_data/data/sogtvt_new_fold_1/vanbansudung_ids.txt  \n","   creating: vietinfo_uitnlp_data/data/sogtvt_new_fold_1/part_1/\n","  inflating: vietinfo_uitnlp_data/data/sogtvt_new_fold_1/part_1/documents.json  \n","  inflating: vietinfo_uitnlp_data/data/sogtvt_new_fold_1/part_1/test.json  \n","  inflating: vietinfo_uitnlp_data/data/sogtvt_new_fold_1/part_1/dev.json  \n","  inflating: vietinfo_uitnlp_data/data/sogtvt_new_fold_1/part_1/train.json  \n","   creating: vietinfo_uitnlp_data/documents/\n","   creating: vietinfo_uitnlp_data/scripts/\n","  inflating: vietinfo_uitnlp_data/scripts/pdf_noise_classifier.pkl  \n","  inflating: vietinfo_uitnlp_data/scripts/split_train_dev_test.py  \n","  inflating: vietinfo_uitnlp_data/scripts/segment_documents.py  \n","  inflating: vietinfo_uitnlp_data/scripts/segment_trichyeu.py  \n","  inflating: vietinfo_uitnlp_data/scripts/create_k_fold.py  \n","  inflating: vietinfo_uitnlp_data/scripts/merge_documents.py  \n","   creating: vietinfo_uitnlp_data/examples/\n","  inflating: vietinfo_uitnlp_data/examples/5_create_k_fold_new.sh  \n","  inflating: vietinfo_uitnlp_data/examples/2_split_train_dev_test_old.sh  \n","  inflating: vietinfo_uitnlp_data/examples/4_merge_documents_new.sh  \n","  inflating: vietinfo_uitnlp_data/examples/3_segment_documents_new.sh  \n","  inflating: vietinfo_uitnlp_data/examples/1_split_train_dev_test_new.sh  \n"]}]},{"cell_type":"code","source":["import torch\n","from torch.utils.data import Dataset, DataLoader, Sampler\n","import numpy as np\n","import pandas as pd\n","import os\n","from torch import nn\n","from tqdm import tqdm"],"metadata":{"id":"2jHfho_5Ivwy"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"j5fRQqjBH4XQ"},"outputs":[],"source":["def pad(tensors, padding_value=0):\n","    size = [len(tensors)] + [max(tensor.size(i) for tensor in tensors)\n","                             for i in range(len(tensors[0].size()))]\n","    out_tensor = tensors[0].data.new(*size).fill_(padding_value)\n","    for i, tensor in enumerate(tensors):\n","        out_tensor[i][[slice(0, i) for i in tensor.size()]] = tensor\n","    return out_tensor\n","\n","\n","class TextDataset(Dataset):\n","    def __init__(self, df, pad_index):\n","        super(TextDataset, self).__init__()\n","        self.df = df\n","        self.col_names = df.columns\n","        self.col_text_names = [col_name for col_name in self.col_names if col_name.startswith('text_')]\n","        self.pad_index = pad_index\n","\n","    def __getitem__(self, index):\n","        for col_name in self.col_names:\n","            yield self.df.iloc[index][col_name]\n","\n","    def __len__(self):\n","        return len(self.df)\n","\n","    @property\n","    def loader(self):\n","        if hasattr(self, 'data_loader'):\n","            return self.data_loader\n","        else:\n","            raise AttributeError\n","\n","    @loader.setter\n","    def loader(self, data_loader):\n","        self.data_loader = data_loader\n","\n","    @classmethod\n","    def collate_fn(cls, batch):\n","        return (field for field in zip(*batch))\n","\n","\n","class TextDataLoader(DataLoader):\n","    def __init__(self, *args, **kwargs):\n","        super(TextDataLoader, self).__init__(*args, **kwargs)\n","\n","        self.col_names = self.dataset.col_names\n","        self.pad_index = self.dataset.pad_index\n","\n","    def __iter__(self):\n","        for raw_batch in super(TextDataLoader, self).__iter__():\n","            batch, device = {}, 'cuda' if torch.cuda.is_available() else 'cpu'\n","            for data, col_name in zip(raw_batch, self.col_names):\n","                if col_name.startswith('text_a'):\n","                    batch[col_name] = pad(data, self.pad_index).to(device)\n","                elif col_name == 'label_a':  \n","                    batch[col_name] = torch.tensor(data).to(device)\n","\n","            yield batch"]},{"cell_type":"code","source":["def read_uit_vsfc(path):\n","    df = pd.read_csv(path)\n","    df = df[df['label'] != 1]\n","    df['label'].replace(2, 1, inplace=True)\n","    df.rename(columns={'label': 'label_a'}, inplace=True)\n","    return df\n","\n","df_train = read_uit_vsfc('train.csv')\n","df_dev = read_uit_vsfc('dev.csv')\n","df_test = read_uit_vsfc('test.csv')\n","\n","idx2word = ['<s>', '</s>', '<pad>', '<unk>']\n","word2idx = {word: idx for idx, word in enumerate(idx2word)}\n","\n","train_words = {}\n","for sentence in df_train['sentence']:\n","    for word in sentence.split(' '):\n","        if word not in train_words: train_words[word] = 1\n","        else: train_words[word] += 1\n","\n","for word in train_words:\n","    if train_words[word] > 1:\n","        if word not in word2idx:\n","            word2idx[word] = len(word2idx)\n","            idx2word.append(word)"],"metadata":{"id":"vDyorV4nxXJ9"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["df_train['text_a'] = df_train['sentence'].apply(lambda inp: torch.tensor([\n","    word2idx[word] if word in word2idx else word2idx['<unk>'] for word in inp.split(' ')\n","]))\n","\n","df_dev['text_a'] = df_dev['sentence'].apply(lambda inp: torch.tensor([\n","    word2idx[word] if word in word2idx else word2idx['<unk>'] for word in inp.split(' ')\n","]))\n","\n","df_test['text_a'] = df_test['sentence'].apply(lambda inp: torch.tensor([\n","    word2idx[word] if word in word2idx else word2idx['<unk>'] for word in inp.split(' ')\n","]))"],"metadata":{"id":"dINlaPO10GZR"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["learning_rate = 1e-3\n","batch_size = 64\n","epochs = 5"],"metadata":{"id":"PoIaR2-c9Jqx"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["dataset_train = TextDataset(df=df_train, pad_index=word2idx['<pad>'])\n","dataloader_train = TextDataLoader(dataset=dataset_train,\n","                                  batch_size=batch_size,\n","                                  shuffle=True,\n","                                  collate_fn=dataset_train.collate_fn,\n","                                  num_workers=0)\n","\n","dataset_dev = TextDataset(df=df_dev, pad_index=word2idx['<pad>'])\n","dataloader_dev = TextDataLoader(dataset=dataset_dev,\n","                                  batch_size=batch_size,\n","                                  shuffle=False,\n","                                  collate_fn=dataset_dev.collate_fn,\n","                                  num_workers=0)\n","\n","dataset_test = TextDataset(df=df_test, pad_index=word2idx['<pad>'])\n","dataloader_test = TextDataLoader(dataset=dataset_test,\n","                                  batch_size=batch_size,\n","                                  shuffle=False,\n","                                  collate_fn=dataset_test.collate_fn,\n","                                  num_workers=0)"],"metadata":{"id":"DzX3PPGAMEpz"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["class Model(nn.Module):\n","    def __init__(self, config):\n","        super(Model, self).__init__()\n","\n","        self.config = config\n","        self.embedding = nn.Embedding(config['num_embeddings'], config['embedding_dim'])\n","        \n","        self.W_x = nn.Linear(config['embedding_dim'], 2*config['embedding_dim'])\n","        self.W_h = nn.Linear(2*config['embedding_dim'], 2*config['embedding_dim'])\n","        self.b = nn.parameter.Parameter(torch.rand(2*config['embedding_dim'], dtype=torch.float32))\n","        self.act_funct = nn.Tanh()\n","\n","        self.h_0 = nn.parameter.Parameter(torch.zeros(2*config['embedding_dim'], dtype=torch.float32))\n","\n","        self.cls = nn.Linear(2*config['embedding_dim'], 1)\n","\n","    def forward(self, batch):\n","        # batch['text_a']: shape: (batch_size, max_seq_length)\n","        # batch['label_a']: shape: (batch_size,)\n","\n","        vec_h = [self.h_0.expand(batch['text_a'].shape[0], 2*self.config['embedding_dim'])]\n","        max_length = batch['text_a'].shape[1]\n","        embed = self.embedding(batch['text_a'])\n","        vec_b = self.b.expand(batch['text_a'].shape[0], 2*self.config['embedding_dim'])\n","        \n","        for i in range(max_length):\n","            vec_h.append(self.act_funct(self.W_x(embed[:, i]) + self.W_h(vec_h[-1]) + vec_b))\n","        vec_h.pop(0)\n","\n","        vec_h = torch.stack(vec_h, dim=1)\n","        mask = batch['text_a'] != self.config['pad_index']\n","        lens = mask.sum(dim=1)\n","\n","        last_vec_h = []\n","        for idx, pos in enumerate(lens):\n","            last_vec_h.append(vec_h[idx, pos - 1, :])\n","\n","        last_vec_h = torch.stack(last_vec_h)\n","\n","        # avg_reps = torch.stack([torch.mean(reps, dim=0) for reps in torch.split(embed[mask], lens.tolist())])\n","        \n","        return torch.sigmoid(self.cls(last_vec_h))"],"metadata":{"id":"LFsFLcNv3gdI"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["config = {\n","    'num_embeddings': len(word2idx),\n","    'embedding_dim': 25,\n","    'pad_index': word2idx['<pad>'],\n","}"],"metadata":{"id":"ktVGkhvP36Hy"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["model = Model(config)\n","loss_func = nn.BCELoss()\n","optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)"],"metadata":{"id":"lcgNBXeS4zdS"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["size = len(dataloader_train.dataset)\n","\n","for epoch in range(epochs):\n","    for batch_idx, batch in enumerate(dataloader_train):\n","        out = torch.squeeze(model(batch))\n","        loss = loss_func(out, batch['label_a'].float())\n","\n","        optimizer.zero_grad()\n","        loss.backward()\n","        optimizer.step()\n","\n","        if batch_idx % 100:\n","            nb_correct = 0\n","            nb_total = 0\n","            for dev_batch in dataloader_dev:\n","                out = torch.squeeze(model(dev_batch)) > 0.5\n","                nb_correct += (dev_batch['label_a'] == out).sum()\n","                nb_total += len(dev_batch['label_a'])\n","                \n","            loss, current = loss.item(), batch_idx * len(batch)\n","            acc = nb_correct/nb_total\n","            print(f\"acc: {acc:>7f}  [{current:>5d}/{size:>5d}]\")"],"metadata":{"id":"p_ayawQzOcPQ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"6RqF1nX3Kdtw"},"execution_count":null,"outputs":[]}]}